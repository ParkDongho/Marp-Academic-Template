@INPROCEEDINGS{albert2019,
  author = {Albert Cohen},
  title = {Polyhedral Compilation as a Design Pattern for Compiler Construction},
  booktitle = {Programming Language Implementation Summer School},
  year = {2019},
  link = {https://pliss2019.github.io/albert_cohen_slides.pdf}
}
@inproceedings{autosa,
author = {Wang, Jie and Guo, Licheng and Cong, Jason},
title = {AutoSA: A Polyhedral Compiler for High-Performance Systolic Arrays on FPGA},
year = {2021},
isbn = {9781450382182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3431920.3439292},
abstract = {While systolic array architectures have the potential to deliver tremendous performance, it is notoriously challenging to customize an efficient systolic array processor for a target application. Designing systolic arrays requires knowledge for both high-level characteristics of the application and low-level hardware details, thus making it a demanding and inefficient process. To relieve users from the manual iterative trial-and-error process, we present AutoSA, an end-to-end compilation framework for generating systolic arrays on FPGA. AutoSA is based on the polyhedral framework, and further incorporates a set of optimizations on different dimensions to boost performance. An efficient and comprehensive design space exploration is performed to search for high-performance designs. We have demonstrated AutoSA on a wide range of applications, on which AutoSA achieves high performance within a short amount of time. As an example, for matrix multiplication, AutoSA achieves 934 GFLOPs, 3.41 TOPs, and 6.95 TOPs in floating point, 16-bit and 8-bit integer data types on Xilinx Alveo U250.},
booktitle = {The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {93–104},
numpages = {12},
keywords = {compilation, fpga, polyhedral model, systolic array},
location = {Virtual Event, USA},
series = {FPGA '21},
link = {http://192.168.0.2:5012/2021/02/17/b0cf4438fdf1d54d4acc427f9bb949645de40c80/},
}
@article{barvinok_manual,
  author = {Verdoolaege, Sven},
  year = {2023},
  month = {04},
  pages = {},
  title = {barvinok: User Guide},
}
@inproceedings{buffets,
  author = {Pellauer, Michael and Shao, Yakun Sophia and Clemons, Jason and Crago, Neal and Hegde, Kartik and Venkatesan, Rangharajan and Keckler, Stephen W. and Fletcher, Christopher W. and Emer, Joel},
  title = {An Efficient and Composable Storage Idiom for Explicit Decoupled Data Orchestration},
  year = {2019},
  isbn = {9781450362405},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3297858.3304025},
  abstract = {Accelerators spend significant area and effort on custom on-chip buffering. Unfortunately, these solutions are strongly tied to particular designs, hampering re-usability across other accelerators or domains. We present buffets, an efficient and composable storage idiom for the needs of accelerators that is independent of any particular design. Buffets have several distinguishing characteristics, including efficient decoupled fills and accesses with fine-grained synchronization, hierarchical composition, and efficient multi-casting. We implement buffets in RTL and show that they only add 2\% control overhead over an 8KB RAM. When compared with DMA-managed double-buffered scratchpads and caches across a range of workloads, buffets improve energy-delay-product by 1.53x and 5.39x, respectively.},
  booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages = {137–151},
  numpages = {15},
  keywords = {synchronization, staging buffers, data orchestration, accelerators},
  location = {Providence, RI, USA},
  series = {ASPLOS '19},
  link = {http://192.168.0.2:5012/2019/04/04/988989673e463db8227e3e4b74bc299ff3ae087c/}
}
@inproceedings{cambricon-s,
  author = {Zhou, Xuda and Du, Zidong and Guo, Qi and Liu, Shaoli and Liu, Chengsi and Wang, Chao and Zhou, Xuehai and Li, Ling and Chen, Tianshi and Chen, Yunji},
  title = {Cambricon-s: addressing irregularity in sparse neural networks through a cooperative software/hardware approach},
  year = {2018},
  isbn = {9781538662403},
  publisher = {IEEE Press},
  link = {https://doi.org/10.1109/MICRO.2018.00011},
  doi = {10.1109/MICRO.2018.00011},
  abstract = {Neural networks have become the dominant algorithms rapidly as they achieve state-of-the-art performance in a broad range of applications such as image recognition, speech recognition and natural language processing. However, neural networks keep moving towards deeper and larger architectures, posing a great challenge to the huge amount of data and computations. Although sparsity has emerged as an effective solution for reducing the intensity of computation and memory accesses directly, irregularity caused by sparsity (including sparse synapses and neurons) prevents accelerators from completely leveraging the benefits; it also introduces costly indexing module in accelerators.In this paper, we propose a cooperative software/hardware approach to address the irregularity of sparse neural networks efficiently. Initially, we observe the local convergence, namely larger weights tend to gather into small clusters during training. Based on that key observation, we propose a software-based coarse-grained pruning technique to reduce the irregularity of sparse synapses drastically. The coarse-grained pruning technique, together with local quantization, significantly reduces the size of indexes and improves the network compression ratio. We further design a hardware accelerator, Cambricon-S, to address the remaining irregularity of sparse synapses and neurons efficiently. The novel accelerator features a selector module to filter unnecessary synapses and neurons. Compared with a state-of-the-art sparse neural network accelerator, our accelerator is 1.71x and 1.37x better in terms of performance and energy efficiency, respectively.},
  booktitle = {Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture},
  pages = {15–28},
  numpages = {14},
  location = {Fukuoka, Japan},
  series = {MICRO-51},
  link = {http://192.168.0.2:5012/2018/10/01/2a84a30a489cd6f59725b80164b24c227d86c160/}
}

@INPROCEEDINGS{cambricon-x,
  author={Zhang, Shijin and Du, Zidong and Zhang, Lei and Lan, Huiying and Liu, Shaoli and Li, Ling and Guo, Qi and Chen, Tianshi and Chen, Yunji},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Cambricon-X: An accelerator for sparse neural networks}, 
  year={2016},
  volume={},
  number={},
  pages={1-12},
  keywords={Artificial neural networks;Neurons;Biological neural networks;Computer architecture;Graphics processing units;Architecture;Feature extraction},
  doi={10.1109/MICRO.2016.7783723},
  link={http://192.168.0.2:5012/2016/10/15/bc20f523a6e97800340e57a94d79926fce05572c/}
}
@INPROCEEDINGS{cambricon_isa,
  author={Liu, Shaoli and Du, Zidong and Tao, Jinhua and Han, Dong and Luo, Tao and Xie, Yuan and Chen, Yunji and Chen, Tianshi},
  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Cambricon: An Instruction Set Architecture for Neural Networks}, 
  year={2016},
  volume={},
  number={},
  pages={393-405},
  keywords={Artificial neural networks;Registers;System-on-chip;Computer architecture;Data transfer;Ground penetrating radar;Libraries},
  doi={10.1109/ISCA.2016.42},
  link={http://192.168.0.2:5012/2016/06/18/da6e8bcf1c92167553a5b383fe6fe5109a4c0321/}
}



@inproceedings{chisel,
  author = {Bachrach, Jonathan and Vo, Huy and Richards, Brian and Lee, Yunsup and Waterman, Andrew and Avi\v{z}ienis, Rimas and Wawrzynek, John and Asanovi\'{c}, Krste},
  title = {Chisel: constructing hardware in a Scala embedded language},
  year = {2012},
  isbn = {9781450311991},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2228360.2228584},
  abstract = {In this paper we introduce Chisel, a new hardware construction language that supports advanced hardware design using highly parameterized generators and layered domain-specific hardware languages. By embedding Chisel in the Scala programming language, we raise the level of hardware design abstraction by providing concepts including object orientation, functional programming, parameterized types, and type inference. Chisel can generate a high-speed C++-based cycle-accurate software simulator, or low-level Verilog designed to map to either FPGAs or to a standard ASIC flow for synthesis. This paper presents Chisel, its embedding in Scala, hardware examples, and results for C++ simulation, Verilog emulation and ASIC synthesis.},
  booktitle = {Proceedings of the 49th Annual Design Automation Conference},
  pages = {1216–1225},
  numpages = {10},
  keywords = {CAD},
  location = {San Francisco, California},
  series = {DAC '12},
  link = {http://192.168.0.2:5012/2012/06/03/021464b67bb87cf6132b2eb5b0c4a61f31ec2775/},
}
@InProceedings{comet,
  author={"Mutlu, Erdal and Tian, Ruiqin and Ren, Bin and Krishnamoorthy, Sriram and Gioiosa, Roberto and Pienaar, Jacques and Kestor, Gokcen"},
  editor={"Chapman, Barbara and Moreira, Jose"},
  title={"COMET: A Domain-Specific Compilation of High-Performance Computational Chemistry"},
  booktitle={"Languages and Compilers for Parallel Computing"},
  year={"2022"},
  publisher={"Springer International Publishing"},
  address={"Cham"},
  pages={"87-103"},
  abstract={"The computational power increases over the past decades have greatly enhanced the ability to simulate chemical reactions and understand ever more complex transformations. Tensor contractions are the fundamental computational building block of these simulations. These simulations have often been tied to one platform and restricted in generality by the interface provided to the user. The expanding prevalence of accelerators and researcher demands necessitate a more general approach which is not tied to specific hardware or requires contortion of algorithms to specific hardware platforms. In this paper we present COMET, a domain-specific programming language and compiler infrastructure for tensor contractions targeting heterogeneous accelerators. We present a system of progressive lowering through multiple layers of abstraction and optimization that achieves up to {\$}{\$}1.98{\backslash}times {\$}{\$}1.98{\texttimes}speedup for 30 tensor contractions commonly used in computational chemistry and beyond."},
  isbn={"978-3-030-95953-1"}
}
@INPROCEEDINGS{constellation,
  author={Zhao, Jerry and Agrawal, Animesh and Nikolic, Borivoje and Asanović, Krste},
  booktitle={2022 15th IEEE/ACM International Workshop on Network on Chip Architectures (NoCArc)}, 
  title={An Open-Source SoC-Capable NoC Generator}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={In response to growing application diversity, System-on-Chip (SoC) architectures have become increasingly heterogeneous, with diverse cores and accelerators, as well as non-uniform memory systems. However, existing open-source design frameworks for SoCs and NoCs (Network-on-Chips) have been unable to facilitate design exploration of heterogeneous SoC architectures with irregular NoCs. We present Constellation, a new NoC RTL generator framework designed from the ground up to support integration in a heterogeneous SoC and evaluation of highly irregular NoC architectures. Constellation implements a highly decoupled specification system that allows an architect to specify an exponentially large design space of irregular virtual-channel wormhole-routed NoC architectures. Additionally, Constellation provides a diverse set of systems, regression tests, and evaluation tools to provide confidence in the correctness and performance of the generated hardware. Constellation is open-sourced and integrated into the Chipyard SoC design framework, allowing full-system exploration of heterogeneous SoC architectures with irregular memory fabrics.},
  keywords={Conferences;Memory architecture;Memory;Generators;Hardware;System-on-chip;network-on-chip;system-on-chip;open-source},
  doi={10.1109/NoCArc57472.2022.9911299},
  ISSN={},
  month={Oct},
  site={/2022/10/02/ieee-9911299/},
}

@INPROCEEDINGS{cosa,
  key={cosa},
  author={Huang, Qijing and Kang, Minwoo and Dinh, Grace and Norell, Thomas and Kalaiah, Aravind and Demmel, James and Wawrzynek, John and Shao, Yakun Sophia},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={CoSA: Scheduling by Constrained Optimization for Spatial Accelerators}, 
  year={2021},
  volume={},
  number={},
  pages={554-566},
  keywords={Schedules;Runtime;Processor scheduling;Navigation;Programming;Search problems;Throughput;scheduling;accelerator;neural networks;compiler optimizations},
  doi={10.1109/ISCA52012.2021.00050},
  link={http://192.168.0.2:5012/2021/05/05/3aef285d724db47d1218c09a0cb7c25573828c4b/},
}

@INPROCEEDINGS{dMazeRunner,
  author={Dave, Shail and Shrivastava, Aviral and Kim, Youngbin and Avancha, Sasikanth and Lee, Kyoungwoo},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={dMazeRunner: Optimizing Convolutions on Dataflow Accelerators}, 
  year={2020},
  volume={},
  number={},
  pages={1544-1548},
  abstract={Convolution neural networks (CNNs) can be efficiently executed on dataflow accelerators. However, the vast space of executing convolutions on computational and memory resources of accelerators makes difficult for programmers to automatically and efficiently accelerate the convolutions and for architects to achieve efficient accelerator designs. We propose dMazeRunner framework, which allows users to optimize execution methods for accelerating convolution and matrix multiplication on a given architecture and to explore dataflow accelerator designs for efficiently executing CNN models. dMazeRunner determines efficient dataflows tailored for CNN layers and achieves efficient execution methods for CNN models within several seconds.},
  keywords={Convolution;Neural networks;Memory management;Hardware;Space exploration;Acceleration;Speech processing;Hardware accelerators;energy-efficiency;mapping;deep learning;design space exploration},
  doi={10.1109/ICASSP40776.2020.9054275},
  ISSN={2379-190X},
  month={May},
  link={http://192.168.0.2:5012/2020/05/01/84c8c7fb017ef3f8a6208837c087c887dc91e4f1/},
}

@INPROCEEDINGS{dadiannao,
  author={Chen, Yunji and Luo, Tao and Liu, Shaoli and Zhang, Shijin and He, Liqiang and Wang, Jia and Li, Ling and Chen, Tianshi and Xu, Zhiwei and Sun, Ninghui and Temam, Olivier},
  booktitle={2014 47th Annual IEEE/ACM International Symposium on Microarchitecture}, 
  title={DaDianNao: A Machine-Learning Supercomputer}, 
  year={2014},
  volume={},
  number={},
  pages={609-622},
  abstract={Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.},
  keywords={Computer architecture;Graphics processing units;Biological neural networks;Neurons;Kernel;Hardware;Bandwidth;machine learning;accelerator;neural network;computer architecture},
  doi={10.1109/MICRO.2014.58},
  ISSN={2379-3155},
  month={Dec},
  link={http://192.168.0.2:5012/2014/12/13/4157ed3db4c656854e69931cb6089b64b08784b9/}
}
@INPROCEEDINGS{dart2016,
  author = {Alain Darte},
  title = {Introduction to Automated Polyhedral Code Optimizations and Tiling},
  booktitle = {Spring School on Numerical Simulation and Polyhedral Code Optimizations},
  year = {2016},
  link = {https://mathsinfohpc.sciencesconf.org/conference/mathsinfohpc/pages/Darte.pdf}
}
@inproceedings{data-dep,
  author = {Dadu, Vidushi and Weng, Jian and Liu, Sihao and Nowatzki, Tony},
  title = {Towards General Purpose Acceleration by Exploiting Common Data-Dependence Forms},
  year = {2019},
  isbn = {9781450369381},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3352460.3358276},
  abstract = {With slowing technology scaling, specialized accelerators are increasingly attractive solutions to continue expected generational scaling of performance. However, in order to accelerate more advanced algorithms or those from challenging domains, supporting data-dependence becomes necessary. This manifests as either data-dependent control (eg. join two sparse lists), or data-dependent memory accesses (eg. hash-table access). These forms of data-dependence inherently couple compute with memory, and also preclude efficient vectorization -- defeating the traditional mechanisms of programmable accelerators (eg. GPUs).Our goal is to develop an accelerator which is broadly applicable across algorithms with and without data-dependence. To this end, we first identify forms of data-dependence which are both common and possible to exploit with specialized hardware: specifically stream-join and alias-free indirection. Then, we create an accelerator with an interface to support these, called the Sparse Processing Unit (SPU). SPU supports alias-free indirection with a compute-enabled scratchpad and aggressive stream reordering and stream-join with a novel dataflow control model for a reconfigurable systolic compute-fabric. Finally, we add robustness across datatypes by adding decomposability across the compute and memory pipelines. SPU achieves 16.5\texttimes{}, 10.3\texttimes{}, and 14.2\texttimes{} over a 24-core SKL CPU on ML, database, and graph algorithms respectively. SPU achieves similar performance to domain-specific accelerators. For ML, SPU achieves 1.8-7\texttimes{} speedup against a similarly provisioned GPGPU, with much less area and power.},
  booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages = {924–939},
  numpages = {16},
  keywords = {Irregularity, accelerators, data-dependence, dataflow, generality, indirection, join, reconfigurable, systolic},
  location = {Columbus, OH, USA},
  series = {MICRO '52}
}

@inproceedings{diannao,
  author = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
  title = {DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning},
  year = {2014},
  isbn = {9781450323055},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2541940.2541967},
  abstract = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.},
  booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages = {269–284},
  numpages = {16},
  keywords = {neural networks, memory, accelerator},
  location = {Salt Lake City, Utah, USA},
  series = {ASPLOS '14},
  link = {http://192.168.0.2:5012/2014/02/24/22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd/},
}

@inproceedings{diplomacy,
  title={Diplomatic design patterns: A TileLink case study},
  author={Cook, Henry and Terpstra, Wesley and Lee, Yunsup},
  booktitle={1st Workshop on Computer Architecture Research with RISC-V},
  volume={23},
  year={2017},
  series = {CARRV'17},
}
@INPROCEEDINGS{dsa-gen,
  author={Weng, Jian and Liu, Sihao and Dadu, Vidushi and Wang, Zhengrong and Shah, Preyas and Nowatzki, Tony},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={DSAGEN: Synthesizing Programmable Spatial Accelerators}, 
  year={2020},
  volume={},
  number={},
  pages={268-281},
  abstract={Domain-specific hardware accelerators can provide orders of magnitude speedup and energy efficiency over general purpose processors. However, they require extensive manual effort in hardware design and software stack development. Automated ASIC generation (eg. HLS) can be insufficient, because the hardware becomes inflexible. An ideal accelerator generation framework would be automatable, enable deep specialization to the domain, and maintain a uniform programming interface. Our insight is that many prior accelerator architectures can be approximated by composing a small number of hardware primitives, specifically those from spatial architectures. With careful design, a compiler can understand how to use available primitives, with modular and composable transformations, to take advantage of the features of a given program. This suggests a paradigm where accelerators can be generated by searching within such a rich accelerator design space, guided by the affinity of input programs for hardware primitives and their interactions. We use this approach to develop the DSAGEN framework, which automates the hardware/software co-design process for reconfigurable accelerators. For several existing accelerators, our evaluation demonstrates that the compiler can achieve 89% of the performance of manually tuned versions. For automated design space exploration, we target multiple sets of workloads which prior accelerators are design for; the generated hardware has mean 1.3× perf2/mm2 over prior programmable accelerators.},
  keywords={},
  doi={10.1109/ISCA45697.2020.00032},
  ISSN={},
  month={May},
  link={http://192.168.0.2:5012/2020/05/30/ieee-9138967/}
}

@ARTICLE{dyser,
  author={Govindaraju, Venkatraman and Ho, Chen-Han and Nowatzki, Tony and Chhugani, Jatin and Satish, Nadathur and Sankaralingam, Karthikeyan and Kim, Changkyu},
  journal={IEEE Micro}, 
  title={DySER: Unifying Functionality and Parallelism Specialization for Energy-Efficient Computing}, 
  year={2012},
  volume={32},
  number={5},
  pages={38-51},
  abstract={The DySER (Dynamically Specializing Execution Resources) architecture supports both functionality specialization and parallelism specialization. By dynamically specializing frequently executing regions and applying parallelism mechanisms, DySER provides efficient functionality and parallelism specialization. It outperforms an out-of-order CPU, Streaming SIMD Extensions (SSE) acceleration, and GPU acceleration while consuming less energy. The full-system field-programmable gate array (FPGA) prototype of DySER integrated into OpenSparc demonstrates a practical implementation.},
  keywords={Parallel processing;Computer architecture;Hardware;Energy efficiency;Field programmable gate arrays;Prototypes;architecture;specialization;data-level parallelism;accelerator;energy efficiency;DySER},
  doi={10.1109/MM.2012.51},
  ISSN={1937-4143},
  month={Sep.},
  link={http://192.168.0.2:5012/2012/09/01/6fbb7db25a5a3a2788e2b16a5e54abc1ac36aa76/},
}

@article{eie,
author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
title = {EIE: efficient inference engine on compressed deep neural network},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5964},
doi = {10.1145/3007787.3001163},
abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power.Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120\texttimes{} energy saving; Exploiting sparsity saves 10\texttimes{}; Weight sharing gives 8\texttimes{}; Skipping zero activations from ReLU saves another 3\texttimes{}. Evaluated on nine DNN benchmarks, EIE is 189\texttimes{} and 13\texttimes{} faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88\texttimes{}104 frames/sec with a power dissipation of only 600mW. It is 24,000\texttimes{} and 3,400\texttimes{} more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9\texttimes{}, 19\texttimes{} and 3\texttimes{} better throughput, energy efficiency and area efficiency.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {243–254},
numpages = {12},
keywords = {model compression, hardware acceleration, deep learning, algorithm-hardware co-design, ASIC},
link = {http://192.168.0.2:5012/2016/02/04/2e2b189f668cf2c06ebc44dc9b166648256cf457/},
}
@inproceedings{ems,
author = {Jia, Liancheng and Wang, Yuyue and Leng, Jingwen and Liang, Yun},
title = {EMS: efficient memory subsystem synthesis for spatial accelerators},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
link = {https://doi.org/10.1145/3489517.3530411},
doi = {10.1145/3489517.3530411},
abstract = {Spatial accelerators provide massive parallelism with an array of homogeneous PEs, and enable efficient data reuse with PE array dataflow and on-chip memory. Many previous works have studied the dataflow architecture of spatial accelerators, including performance analysis and automatic generation. However, existing accelerator generators fail to exploit the entire memory-level reuse opportunities, and generate suboptimal designs with data duplication and inefficient interconnection.In this paper, we propose EMS, an efficient memory subsystem synthesis and optimization framework for spatial accelerators. We first use space-time transformation (STT) to analyze both PE-level and memory-level data reuse. Based on the reuse analysis, we develop an algorithm to automatically generate data layout of the multi-banked scratchpad memory, data mapping, and access controller for the memory. Our generated memory subsystem supports multiple PE-memory interconnection topologies including direct, multicast, and rotated connection. The memory and interconnection generation approach can efficiently utilize the memory-level reuse to avoid duplicated data storage with low hardware cost. EMS can automatically synthesize tensor algebra to hardware designed in Chisel. Experiments show that our proposed memory generator reduces the on-chip memory size by an average of 28\% than the state-of-the-art, and achieves comparable hardware performance.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {67–72},
numpages = {6},
keywords = {spatial accelerators, space-time transformation, memory},
location = {San Francisco, California},
series = {DAC '22}
}
@INPROCEEDINGS{eyeriss_isscc,
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
  booktitle={2016 IEEE International Solid-State Circuits Conference (ISSCC)}, 
  title={An energy-efficient reconfigurable accelerator for deep convolutional neural networks}, 
  year={2016},
  volume={},
  number={},
  pages={262-263},
  keywords={Shape;Random access memory;Arrays;Clocks;Logic arrays;Memory management;Bandwidth},
  doi={10.1109/ISSCC.2016.7418007},
}

@ARTICLE{eyeriss_jscc,
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}, 
  year={2017},
  volume={52},
  number={1},
  pages={127-138},
  abstract={Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs). It optimizes for the energy efficiency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconfiguring the architecture. CNNs are widely used in modern AI systems but also bring challenges on throughput and energy efficiency to the underlying hardware. This is because its computation requires a large amount of data, creating significant data movement from on-chip and off-chip that is more energy-consuming than computation. Minimizing data movement energy cost for any CNN shape, therefore, is the key to high throughput and energy efficiency. Eyeriss achieves these goals by using a proposed processing dataflow, called row stationary (RS), on a spatial architecture with 168 processing elements. RS dataflow reconfigures the computation mapping of a given shape, which optimizes energy efficiency by maximally reusing data locally to reduce expensive data movement, such as DRAM accesses. Compression and data gating are also applied to further improve energy efficiency. Eyeriss processes the convolutional layers at 35 frames/s and 0.0029 DRAM access/multiply and accumulation (MAC) for AlexNet at 278 mW (batch size N = 4), and 0.7 frames/s and 0.0035 DRAM access/MAC for VGG-16 at 236 mW (N = 3).},
  keywords={Shape;Random access memory;Computer architecture;Throughput;Clocks;Neural networks;Hardware;Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture},
  doi={10.1109/JSSC.2016.2616357},
  ISSN={1558-173X},
  month={Jan},
  link={http://192.168.0.2:5012/2016/02/01/ffdaa12ef011de9dbf43be46d45a3abcc8288965/},
}
@ARTICLE{eyeriss_v2,
  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
  title={Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices}, 
  year={2019},
  volume={9},
  number={2},
  pages={292-308},
  abstract={A recent trend in deep neural network (DNN) development is to extend the reach of deep learning applications to platforms that are more resource and energy-constrained, e.g., mobile devices. These endeavors aim to reduce the DNN model size and improve the hardware processing efficiency and have resulted in DNNs that are much more compact in their structures and/or have high data sparsity. These compact or sparse models are different from the traditional large ones in that there is much more variation in their layer shapes and sizes and often require specialized hardware to exploit sparsity for performance improvement. Therefore, many DNN accelerators designed for large DNNs do not perform well on these models. In this paper, we present Eyeriss v2, a DNN accelerator architecture designed for running compact and sparse DNNs. To deal with the widely varying layer shapes and sizes, it introduces a highly flexible on-chip network, called hierarchical mesh, that can adapt to the different amounts of data reuse and bandwidth requirements of different data types, which improves the utilization of the computation resources. Furthermore, Eyeriss v2 can process sparse data directly in the compressed domain for both weights and activations and therefore is able to improve both processing speed and energy efficiency with sparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65-nm CMOS process achieves a throughput of 1470.6 inferences/s and 2560.3 inferences/J at a batch size of 1, which is 12.6× faster and 2.5× more energyefficient than the original Eyeriss running MobileNet.},
  keywords={Hardware;Shape;Arrays;Parallel processing;Mobile handsets;Bandwidth;Deep neural network accelerators;deep learning;energy-efficient accelerators;dataflow processing;spatial architecture},
  doi={10.1109/JETCAS.2019.2910232},
  ISSN={2156-3365},
  month={June},
  link={http://192.168.0.2:5012/2018/07/10/0682bfa5cca15726aab6c00ecfac91eb44379626/},
}

@article{feautrier,
  author = {Feautrier, Paul},
  title = {Some efficient solutions to the affine scheduling problem: I. one-dimensional time},
  year = {1992},
  issue_date = {Oct. 1992},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {21},
  number = {5},
  issn = {0885-7458},
  link = {https://doi.org/10.1007/BF01407835},
  doi = {10.1007/BF01407835},
  journal = {Int. J. Parallel Program.},
  month = {oct},
  pages = {313–348},
  numpages = {36},
  keywords = {automatic parallelization, automatic systolic array design, scheduling},
}
@inproceedings{fkcc,
  author = {Alias, Christophe},
  title = {The Farkas Calculator},
  year = {2019},
  isbn = {978-3-030-54996-1},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  link = {https://doi.org/10.1007/978-3-030-54997-8_32},
  doi = {10.1007/978-3-030-54997-8_32},
  abstract = {In this paper, we present fkcc, a scripting tool to prototype program analyses and transformations exploiting the affine form of Farkas lemma. Our language is general enough to prototype in a few lines sophisticated termination and scheduling algorithms. The tool is freely available and may be tried online via a web interface. We believe that fkcc is the missing chain to accelerate the development of program analyses and transformations exploiting the affine form of Farkas lemma.},
  booktitle = {Formal Methods. FM 2019 International Workshops: Porto, Portugal, October 7–11, 2019, Revised Selected Papers, Part II},
  pages = {526–536},
  numpages = {11},
  keywords = {Scheduling, Termination, Scripting tool, Farkas lemma},
  location = {Porto, Portugal}
}
@INPROCEEDINGS {fkcc_impact,
  author = {Alias, Christophe},
  title = {Farkas Lemma made easy},
  booktitle = {IMPACT 2020: International workshop on polyhedral compilation techniques},
  year = {2020},
  pages = {1-6},
  numpages = {6},
  link = {https://inria.hal.science/hal-02422033/file/final.pdf}
}
@INPROCEEDINGS{flexflow,
  author={Lu, Wenyan and Yan, Guihai and Li, Jiajun and Gong, Shijun and Han, Yinhe and Li, Xiaowei},
  booktitle={2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks}, 
  year={2017},
  volume={},
  number={},
  pages={553-564},
  abstract={Convolutional Neural Networks (CNN) are very computation-intensive. Recently, a lot of CNN accelerators based on the CNN intrinsic parallelism are proposed. However, we observed that there is a big mismatch between the parallel types supported by computing engine and the dominant parallel types of CNN workloads. This mismatch seriously degrades resource utilization of existing accelerators. In this paper, we propose a flexible dataflow architecture (FlexFlow) that can leverage the complementary effects among feature map, neuron, and synapse parallelism to mitigate the mismatch. We evaluated our design with six typical practical workloads, it acquires 2-10x performance speedup and 2.5-10x power efficiency improvement compared with three state-of-the-art accelerator architectures. Meanwhile, FlexFlow is highly scalable with growing computing engine scale.},
  keywords={Parallel processing;Neurons;Computer architecture;Kernel;Clocks;Pipelines;Biological neural networks;Flexible Dataflow;Complementary Effect;Convolutional Neural Networks;Accelerator},
  doi={10.1109/HPCA.2017.29},
  ISSN={2378-203X},
  month={Feb},
}

@INPROCEEDINGS{fused-layer,
  author={Alwani, Manoj and Chen, Han and Ferdman, Michael and Milder, Peter},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={Fused-layer CNN accelerators}, 
  year={2016},
  volume={},
  number={},
  pages={1-12},
  keywords={System-on-chip;Bandwidth;Random access memory;Neural networks;Convolution;Field programmable gate arrays;Data transfer},
  doi={10.1109/MICRO.2016.7783725},
  link={http://192.168.0.2:5012/2016/10/15/72ed74f00d0f7312f7ed96d93ed43f0052d526bc/},
}
@inproceedings{gamma,
author = {Kao, Sheng-Chun and Krishna, Tushar},
title = {GAMMA: automating the HW mapping of DNN models on accelerators via genetic algorithm},
year = {2020},
isbn = {9781450380263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3400302.3415639},
abstract = {DNN layers are multi-dimensional loops that can be ordered, tiled, and scheduled in myriad ways across space and time on DNN accelerators. Each of these choices is called a mapping. It has been shown that the mapping plays an extremely crucial role in overall performance and efficiency, as it directly determines the amount of reuse that the accelerator can leverage from the DNN. Moreover, instead of using a fixed mapping for every DNN layer, research has revealed the benefit of optimizing per-layer mappings. However, determining the right mapping, given an accelerator and layer is still an open question. The immense space of mappings (or map-space) makes brute-forced exhaustive search methods unapproachable. In this paper, we propose a domain-specific genetic algorithm-based method, GAMMA, which is specially designed for this HW-mapping problem. In contrast to prior works that either target simple rigid accelerators with a limited map-space or choose from a restricted set of mappings, we construct an extremely flexible map-space and show that GAMMA can explore the space and determine an optimized mapping with high sample efficiency. We quantitatively compare GAMMA with many popular optimization methods and observe GAMMA consistently finds better solutions.},
booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
articleno = {44},
numpages = {9},
keywords = {ML accelerator, genetic algorithm, reconfigurable device},
location = {Virtual Event, USA},
series = {ICCAD '20},
link = {http://192.168.0.2:5012/2020/11/02/78211429633a99c5ca1b66ba0a7bc6def79c7f40/},
}
@INPROCEEDINGS{gemmini-dac,
  author={Genc, Hasan and Kim, Seah and Amid, Alon and Haj-Ali, Ameer and Iyer, Vighnesh and Prakash, Pranav and Zhao, Jerry and Grubb, Daniel and Liew, Harrison and Mao, Howard and Ou, Albert and Schmidt, Colin and Steffl, Samuel and Wright, John and Stoica, Ion and Ragan-Kelley, Jonathan and Asanovic, Krste and Nikolic, Borivoje and Shao, Yakun Sophia},
  booktitle={Proceedings of the 58th Annual Design Automation Conference (DAC)}, 
  title={Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration}, 
  year={2021},
  volume={},
  number={},
  pages={}
}
@inproceedings{halide,
author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}do and Amarasinghe, Saman},
title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
link = {https://doi.org/10.1145/2491956.2462176},
doi = {10.1145/2491956.2462176},
abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {519–530},
numpages = {12},
keywords = {vectorization, redundant computation, parallelism, optimization, locality, image processing, gpu, domain specific language, compiler, autotuning},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}
@INPROCEEDINGS{hasco,
  author={Xiao, Qingcheng and Zheng, Size and Wu, Bingzhe and Xu, Pengcheng and Qian, Xuehai and Liang, Yun},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation}, 
  year={2021},
  volume={},
  number={},
  pages={1055-1068},
  abstract={Tensor computations overwhelm traditional general-purpose computing devices due to the large amounts of data and operations of the computations. They call for a holistic solution composed of both hardware acceleration and software mapping. Hardware/software (HW/SW) co-design optimizes the hardware and software in concert and produces high-quality solutions. There are two main challenges in the co-design flow. First, multiple methods exist to partition tensor computation and have different impacts on performance and energy efficiency. Besides, the hardware part must be implemented by the intrinsic functions of spatial accelerators. It is hard for programmers to identify and analyze the partitioning methods manually. Second, the overall design space composed of HW/SW partitioning, hardware optimization, and software optimization is huge. The design space needs to be efficiently explored. To this end, we propose an agile co-design approach HASCO that provides an efficient HW/SW solution to dense tensor computation. We use tensor syntax trees as the unified IR, based on which we develop a two-step approach to identify partitioning methods. For each method, HASCO explores the hardware and software design spaces. We propose different algorithms for the explorations, as they have distinct objectives and evaluation costs. Concretely, we develop a multi-objective Bayesian optimization algorithm to explore hardware optimization. For software optimization, we use heuristic and Q-learning algorithms. Experiments demonstrate that HASCO achieves a 1.25X to 1.44X latency reduction through HW/SW co-design compared with developing the hardware and software separately.},
  keywords={Tensors;Heuristic algorithms;Software algorithms;Syntactics;Hardware;Software;Partitioning algorithms;Tensor Computation;hardware-software-co-design},
  doi={10.1109/ISCA52012.2021.00086},
  ISSN={2575-713X},
  month={June},
  link={http://192.168.0.2:5012/2021/05/04/fc27f18b50a95b6c07ccb517cd5c385582d9a4cf/},
}

@inproceedings{heteroCL,
author = {Lai, Yi-Hsiang and Chi, Yuze and Hu, Yuwei and Wang, Jie and Yu, Cody Hao and Zhou, Yuan and Cong, Jason and Zhang, Zhiru},
title = {HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing},
year = {2019},
isbn = {9781450361378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3289602.3293910},
abstract = {With the pursuit of improving compute performance under strict power constraints, there is an increasing need for deploying applications to heterogeneous hardware architectures with accelerators, such as GPUs and FPGAs. However, although these heterogeneous computing platforms are becoming widely available, they are very difficult to program especially with FPGAs. As a result, the use of such platforms has been limited to a small subset of programmers with specialized hardware knowledge. To tackle this challenge, we introduce HeteroCL, a programming infrastructure composed of a Python-based domain-specific language (DSL) and an FPGA-targeted compilation flow. The HeteroCL DSL provides a clean programming abstraction that decouples algorithm specification from three important types of hardware customization in compute, data types, and memory architectures. HeteroCL further captures the interdependence among these different customization techniques, allowing programmers to explore various performance/area/accuracy trade-offs in a systematic and productive manner. In addition, our framework produces highly efficient hardware implementations for a variety of popular workloads by targeting spatial architecture templates such as systolic arrays and stencil with dataflow architectures. Experimental results show that HeteroCL allows programmers to explore the design space efficiently in both performance and accuracy by combining different types of hardware customization and targeting spatial architectures, while keeping the algorithm code intact.},
booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {242–251},
numpages = {10},
keywords = {compiler, domain-specific language, fpga, hardware accelerator, high-level synthesis, multi-paradigm programming, python, reconfigurable computing, spatial architecture, stencil, systolic array},
location = {Seaside, CA, USA},
series = {FPGA '19},
link = {http://192.168.0.2:5012/2019/02/20/33715e08c111625427ddea46f3da52bba090e6da/},
}
@inproceedings{interstellar,
author = {Yang, Xuan and Gao, Mingyu and Liu, Qiaoyi and Setter, Jeff and Pu, Jing and Nayak, Ankita and Bell, Steven and Cao, Kaidi and Ha, Heonjae and Raina, Priyanka and Kozyrakis, Christos and Horowitz, Mark},
title = {Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
link = {https://doi.org/10.1145/3373376.3378514},
doi = {10.1145/3373376.3378514},
abstract = {We show that DNN accelerator micro-architectures and their program mappings represent specific choices of loop order and hardware parallelism for computing the seven nested loops of DNNs, which enables us to create a formal taxonomy of all existing dense DNN accelerators. Surprisingly, the loop transformations needed to create these hardware variants can be precisely and concisely represented by Halide's scheduling language. By modifying the Halide compiler to generate hardware, we create a system that can fairly compare these prior accelerators. As long as proper loop blocking schemes are used, and the hardware can support mapping replicated loops, many different hardware dataflows yield similar energy efficiency with good performance. This is because the loop blocking can ensure that most data references stay on-chip with good locality and the processing units have high resource utilization. How resources are allocated, especially in the memory system, has a large impact on energy and performance. By optimizing hardware resource allocation while keeping throughput constant, we achieve up to 4.2X energy improvement for Convolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long Short-Term Memories (LSTMs) and multi-layer perceptrons (MLPs), respectively.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {369–383},
numpages = {15},
keywords = {dataflow, domain specific language, neural networks},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}
@INPROCEEDINGS {iscc,
  author = {Sven Verdoolaege},
  title = {Counting Affine Calculator and Applications},
  booktitle = {IMPACT 2011: First international workshop on polyhedral compilation techniques},
  year = {2011},
  pages = {1-6},
  numpages = {6},
  link = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-05.pdf}
}
@INPROCEEDINGS {iscc_tutorial,
  author = {Sven Verdoolaege},
  title = {iscc Tutorial},
  year = {2011},
  link = {https://www.cs.colostate.edu/~pouchet/lectures/doc/iscc-tutorial.pdf}
}
@article{libisl_manual,
  author = {Verdoolaege, Sven},
  year = {2023},
  month = {04},
  pages = {},
  title = {Integer Set Library: Manual},
}
@article{libisl_tutorial,
  author = {Verdoolaege, Sven},
  year = {2016},
  month = {01},
  pages = {},
  title = {Presburger Formulas and Polyhedral Compilation},
}
@inproceedings{maeri,
author = {Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar},
title = {MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3173162.3173176},
abstract = {Deep neural networks (DNN) have demonstrated highly promising results across computer vision and speech recognition, and are becoming foundational for ubiquitous AI. The computational complexity of these algorithms and a need for high energy-efficiency has led to a surge in research on hardware accelerators. \% for this paradigm. To reduce the latency and energy costs of accessing DRAM, most DNN accelerators are spatial in nature, with hundreds of processing elements (PE) operating in parallel and communicating with each other directly. DNNs are evolving at a rapid rate, and it is common to have convolution, recurrent, pooling, and fully-connected layers with varying input and filter sizes in the most recent topologies.They may be dense or sparse. They can also be partitioned in myriad ways (within and across layers) to exploit data reuse (weights and intermediate outputs). All of the above can lead to different dataflow patterns within the accelerator substrate. Unfortunately, most DNN accelerators support only fixed dataflow patterns internally as they perform a careful co-design of the PEs and the network-on-chip (NoC). In fact, the majority of them are only optimized for traffic within a convolutional layer. This makes it challenging to map arbitrary dataflows on the fabric efficiently, and can lead to underutilization of the available compute resources. DNN accelerators need to be programmable to enable mass deployment. For them to be programmable, they need to be configurable internally to support the various dataflow patterns that could be mapped over them. To address this need, we present MAERI, which is a DNN accelerator built with a set of modular and configurable building blocks that can easily support myriad DNN partitions and mappings by appropriately configuring tiny switches. MAERI provides 8-459\% better utilization across multiple dataflow mappings over baselines with rigid NoC fabrics.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {461–475},
numpages = {15},
keywords = {convolutional neural network, deep learning accelerator, machine learning, network-on-chip, recurrent neural network, spatial architecture},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18},
link = {http://192.168.0.2:5012/2018/03/19/5f0da3cedda449b72fe36fa78798651a038f515c/},
}
@inproceedings{maestro_micro,
author = {Kwon, Hyoukjun and Chatarasi, Prasanth and Pellauer, Michael and Parashar, Angshuman and Sarkar, Vivek and Krishna, Tushar},
title = {Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3352460.3358252},
abstract = {The data partitioning and scheduling strategies used by DNN accelerators to leverage reuse and perform staging are known as dataflow, which directly impacts the performance and energy efficiency of DNN accelerators. An accelerator micro architecture dictates the dataflow(s) that can be employed to execute layers in a DNN. Selecting a dataflow for a layer can have a large impact on utilization and energy efficiency, but there is a lack of understanding on the choices and consequences of dataflow, and of tools and methodologies to help architects explore the co-optimization design space.In this work, we first introduce a set of data-centric directives to concisely specify the DNN dataflow space in a compiler-friendly form. We then show how these directives can be analyzed to infer various forms of reuse and to exploit them using hardware capabilities. We codify this analysis into an analytical cost model, MAESTRO (Modeling Accelerator Efficiency via Patio-Temporal Reuse and Occupancy), that estimates various cost-benefit tradeoffs of a dataflow including execution time and energy efficiency for a DNN model and hardware configuration. We demonstrate the use of MAESTRO to drive a hardware design space exploration experiment, which searches across 480M designs to identify 2.5M valid designs at an average rate of 0.17M designs per second, including Pareto-optimal throughput- and energy-optimized design points.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {754–768},
numpages = {15},
keywords = {Cost modeling, Dataflow, Neural networks},
location = {Columbus, OH, USA},
series = {MICRO '52},
link = {http://192.168.0.2:5012/2018/05/04/1f0bbcbcea15b60b39012e9aedf4dac42dff9411/}
}
@article{marvel,
  author = {Chatarasi, Prasanth and Kwon, Hyoukjun and Parashar, Angshuman and Pellauer, Michael and Krishna, Tushar and Sarkar, Vivek},
  title = {Marvel: A Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators},
  year = {2021},
  issue_date = {March 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {19},
  number = {1},
  issn = {1544-3566},
  link = {https://doi.org/10.1145/3485137},
  doi = {10.1145/3485137},
  abstract = {A spatial accelerator’s efficiency depends heavily on both its mapper and cost models to generate optimized mappings for various operators of DNN models. However, existing cost models lack a formal boundary over their input programs (operators) for accurate and tractable cost analysis of the mappings, and this results in adaptability challenges to the cost models for new operators. We consider the recently introduced Maestro Data-Centric (MDC) notation and its analytical cost model to address this challenge because any mapping expressed in the notation is precisely analyzable using the MDC’s cost model.In this article, we characterize the set of input operators and their mappings expressed in the MDC notation by introducing a set of conformability rules. The outcome of these rules is that any loop nest that is perfectly nested with affine tensor subscripts and without conditionals is conformable to the MDC notation. A majority of the primitive operators in deep learning are such loop nests. In addition, our rules enable us to automatically translate a mapping expressed in the loop nest form to MDC notation and use the MDC’s cost model to guide upstream mappers. Our conformability rules over the input operators result in a structured mapping space of the operators, which enables us to introduce a mapper based on our decoupled off-chip/on-chip approach to accelerate mapping space exploration. Our mapper decomposes the original higher-dimensional mapping space of operators into two lower-dimensional off-chip and on-chip subspaces and then optimizes the off-chip subspace followed by the on-chip subspace. We implemented our overall approach in a tool called Marvel, and a benefit of our approach is that it applies to any operator conformable with the MDC notation. We evaluated Marvel over major DNN operators and compared it with past optimizers.},
  journal = {ACM Trans. Archit. Code Optim.},
  month = {dec},
  articleno = {6},
  numpages = {26},
  keywords = {mappers/optimizers, compilers, Deep learning (spatial) accelerators}
}
@inproceedings{mind-mapping,
author = {Hegde, Kartik and Tsai, Po-An and Huang, Sitao and Chandra, Vikas and Parashar, Angshuman and Fletcher, Christopher W.},
title = {Mind mappings: enabling efficient algorithm-accelerator mapping space search},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
link = {https://doi.org/10.1145/3445814.3446762},
doi = {10.1145/3445814.3446762},
abstract = {Modern day computing increasingly relies on specialization to satiate growing performance and efficiency requirements. A core challenge in designing such specialized hardware architectures is how to perform mapping space search, i.e., search for an optimal mapping from algorithm to hardware. Prior work shows that choosing an inefficient mapping can lead to multiplicative-factor efficiency overheads. Additionally, the search space is not only large but also non-convex and non-smooth, precluding advanced search techniques. As a result, previous works are forced to implement mapping space search using expert choices or sub-optimal search heuristics. This work proposes Mind Mappings, a novel gradient-based search method for algorithm-accelerator mapping space search. The key idea is to derive a smooth, differentiable approximation to the otherwise non-smooth, non-convex search space. With a smooth, differentiable approximation, we can leverage efficient gradient-based search algorithms to find high-quality mappings. We extensively compare Mind Mappings to black-box optimization schemes used in prior work. When tasked to find mappings for two important workloads (CNN and MTTKRP), Mind Mapping finds mappings that achieve an average 1.40\texttimes{}, 1.76\texttimes{}, and 1.29\texttimes{}&nbsp;(when run for a fixed number of steps) and 3.16\texttimes{}, 4.19\texttimes{}, and 2.90\texttimes{}&nbsp;(when run for a fixed amount of time) better energy-delay product (EDP) relative to Simulated Annealing, Genetic Algorithms and Reinforcement Learning, respectively. Meanwhile, Mind Mappings returns mappings with only 5.32\texttimes{} higher EDP than a possibly unachievable theoretical lower-bound, indicating proximity to the global optima.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {943–958},
numpages = {16},
keywords = {gradient-based search, mapping space search, programmable domain-specific accelerators},
location = {Virtual, USA},
series = {ASPLOS '21}
}
@INPROCEEDINGS{mlir,
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={MLIR: Scaling Compiler Infrastructure for Domain Specific Computation}, 
  year={2021},
  volume={},
  number={},
  pages={2-14},
  keywords={Program processors;Buildings;Semantics;Hardware;Software;Generators;Optimization},
  doi={10.1109/CGO51591.2021.9370308}
}

@article{nvdla,
  title={Nvdla: Using an instruction interface for compiling deep learning applications to the NVIDIA Deep Learning Accelerator},
  author={Dhome-Casanova, Thomas},
  year={2023}
}
@INPROCEEDINGS{openSMART,
  author={Kwon, Hyoukjun and Krishna, Tushar},
  booktitle={2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={OpenSMART: Single-cycle multi-hop NoC generator in BSV and Chisel}, 
  year={2017},
  volume={},
  number={},
  pages={195-204},
  abstract={The chip industry faces two key challenges today - the impending end of Moore's Law and the rising costs of chip design and verification (millions of dollars today). Heterogeneous IPs - cores and domain-specific accelerators - are a promising answer to the first challenge, enabling performance and energy benefits no longer provided by technology scaling. IP-reuse with plug-and-play designs can help with the second challenge, amortizing NRE costs tremendously. A key requirement in a heterogeneous IP-based plug-and-play SoC environment is an interconnection fabric to connect these IPs together. This fabric needs to be scalable - low latency, low energy and low area - and yet be flexible/parametrizable for use across designs. The key scalability challenge in any Network-on-Chip (NoC) today is that the latency increases proportional to the number of hops. In this work, we present a NoC generator called OpenSMART, which generates low-latency NoCs based on SMART1. SMART is a recently proposed NoC microarchitecture that enables multihop on-chip traversals within a single cycle, removing the dependence of latency on hops. SMART leverages wire delay of the underlying repeated wires, and augments each router with the ability to request and setup bypass paths. OpenSMART takes SMART from a NoC optimization to a design methodology for SoCs, enabling users to generate verified RTL for a class of userspecified network configurations, such as network size, topology, routing algorithm, number of VCs/buffers, router pipeline stages, and so on. OpenSMART also provides the ability to generate any heterogeneous topology with low and high-radix routers and optimized single-stage pipelines, leveraging fast logic delays in technology nodes today. OpenSMART v1.0 comes with both Bluespec System Verilog and Chisel implementations, and this paper also presents a case study of our experiences with both languages. OpenSMART is available for download2 and is going to be a key addition to the emerging open-source hardware movement, providing a glue for interconnecting existing and emerging IPs.},
  keywords={Hardware design languages;Generators;Topology;IP networks;Wires;Network topology;Hardware},
  doi={10.1109/ISPASS.2017.7975291},
  ISSN={},
  month={April},
  site={/2017/04/24/ieee-7975291/},
}
@inproceedings{openSoCFabric,
author = {Fatollahi-Fard, Farzad and Donofrio, David and Michelogiannakis, George and Shalf, John},
title = {OpenSoC Fabric: On-Chip Network Generator: Using Chisel to Generate a Parameterizable On-Chip Interconnect Fabric},
year = {2014},
isbn = {9781450330640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
link = {https://doi.org/10.1145/2685342.2685351},
doi = {10.1145/2685342.2685351},
abstract = {Recent advancements in technology scaling have sparked a trend towards greater integration with large-scale chips containing thousands of processors connected to memories and other I/O devices using non-trivial network topologies. Software simulation suffers from long execution times or reduced accuracy in such complex systems, whereas hardware RTL development is too time-consuming. We present OpenSoC Fabric, a parameterizable and powerful on-chip network generator for evaluating future large-scape chip multiprocessors and SoCs. OpenSoC Fabric leverages a new hardware DSL, Chisel, which contains powerful abstractions provided by its base language, Scala, and generates both software (C++) and hardware (Verilog) models from a single code base. This is in contrast to other tools readily available which typically provide either software or hardware models, but not both. The OpenSoC Fabric infrastructure is modeled after existing state-of-the-art simulators, offers large and powerful collections of configuration options, is open-source, and uses object-oriented design and functional programming to make functionality extension as easy as possible.},
booktitle = {Proceedings of the 2014 International Workshop on Network on Chip Architectures},
pages = {45–50},
numpages = {6},
keywords = {Chisel, Emulation, FPGA, Modeling, Network-on-Chip, Simulation},
location = {Cambridge, United Kingdom},
series = {NoCArc '14}
}
@INPROCEEDINGS{outerspace,
  author={Pal, Subhankar and Beaumont, Jonathan and Park, Dong-Hyeon and Amarnath, Aporva and Feng, Siying and Chakrabarti, Chaitali and Kim, Hun-Seok and Blaauw, David and Mudge, Trevor and Dreslinski, Ronald},
  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator}, 
  year={2018},
  volume={},
  number={},
  pages={724-736},
  abstract={Sparse matrices are widely used in graph and data analytics, machine learning, engineering and scientific applications. This paper describes and analyzes OuterSPACE, an accelerator targeted at applications that involve large sparse matrices. OuterSPACE is a highly-scalable, energy-efficient, reconfigurable design, consisting of massively parallel Single Program, Multiple Data (SPMD)-style processing units, distributed memories, high-speed crossbars and High Bandwidth Memory (HBM). We identify redundant memory accesses to non-zeros as a key bottleneck in traditional sparse matrix-matrix multiplication algorithms. To ameliorate this, we implement an outer product based matrix multiplication technique that eliminates redundant accesses by decoupling multiplication from accumulation. We demonstrate that traditional architectures, due to limitations in their memory hierarchies and ability to harness parallelism in the algorithm, are unable to take advantage of this reduction without incurring significant overheads. OuterSPACE is designed to specifically overcome these challenges. We simulate the key components of our architecture using gem5 on a diverse set of matrices from the University of Florida's SuiteSparse collection and the Stanford Network Analysis Project and show a mean speedup of 7.9× over Intel Math Kernel Library on a Xeon CPU, 13.0× against cuSPARSE and 14.0× against CUSP when run on an NVIDIA K40 GPU, while achieving an average throughput of 2.9 GFLOPS within a 24 W power budget in an area of 87 mm2.},
  keywords={Sparse matrices;Computer architecture;Kernel;Matrix decomposition;Parallel processing;Graphics processing units;Libraries;Sparse matrix processing;Application specific hardware;Parallel computer architecture;Hardware software co design;Hardware accelerators},
  doi={10.1109/HPCA.2018.00067},
  ISSN={2378-203X},
  month={Feb},
  link={http://192.168.0.2:5012/2018/03/27/061edcbe54d23e6c8688525503ebc643c30d44a0/}
}

@inproceedings{pluto,
  author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
  title = {A practical automatic polyhedral parallelizer and locality optimizer},
  year = {2008},
  isbn = {9781595938602},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  link = {https://doi.org/10.1145/1375581.1375595},
  doi = {10.1145/1375581.1375595},
  abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model -- far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
  booktitle = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages = {101–113},
  numpages = {13},
  keywords = {affine transformations, automatic parallelization, locality optimization, loop transformations, polyhedral model, tiling},
  location = {Tucson, AZ, USA},
  series = {PLDI '08},
  link = {http://192.168.0.2:5012/2008/06/07/f0f4757aa2f923a349e8357e73850a78e9b80fee/}
}
@inproceedings{polysa,
  author = {Cong, Jason and Wang, Jie},
  title = {PolySA: Polyhedral-Based Systolic Array Auto-Compilation},
  year = {2018},
  publisher = {IEEE Press},
  link = {https://doi.org/10.1145/3240765.3240838},
  doi = {10.1145/3240765.3240838},
  abstract = {Automatic systolic array generation has long been an interesting topic due to the need to reduce the lengthy development cycles of manual designs. Existing automatic systolic array generation approach builds dependency graphs from algorithms, and iteratively maps computation nodes in the graph into processing elements (PEs) with time stamps that specify the sequences of nodes that operate within the PE. There are a number of previous works that implemented the idea and generated designs for ASICs. However, all of these works relied on human intervention and usually generated inferior designs compared to manual designs. In this work, we present our ongoing compilation framework named PolySA which leverages the power of the polyhedral model to achieve the end-to-end compilation for systolic array architecture on FPGAs. PolySA is the first fully automated compilation framework for generating high-performance systolic array architectures on the FPGA leveraging recent advances in high-level synthesis. We demonstrate PolySA on two key applications–matrix multiplication and convolutional neural network. PolySA is able to generate optimal designs within one hour with performance comparable to state-of-the-art manual designs.},
  booktitle = {2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages = {1–8},
  numpages = {8},
  location = {San Diego, CA, USA},
  link = {http://192.168.0.2:5012/2018/11/05/371fd9c263ea0ef1c65e547ce4f90741ed90f10a/},
}
@INPROCEEDINGS{pom,
  author={Zhang, Weichuang and Zhao, Jieru and Shen, Guan and Chen, Quan and Chen, Chen and Guo, Minyi},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={An Optimizing Framework on MLIR for Efficient FPGA-based Accelerator Generation}, 
  year={2024},
  volume={},
  number={},
  pages={75-90},
  abstract={With the increasing demand for computing capability given limited resource and power budgets, it is prominent to deploy applications to customized accelerators like FPGAs. However, FPGA programming is non-trivial. Although existing high-level synthesis (HLS) tools improve productivity to a certain extent, they are limited in scope and capability to support sufficient FPGA-oriented transformations and optimizations. This paper focuses on FPGA-based accelerators and proposes POM, an end-to-end optimizing framework built on multi-level intermediate representation (MLIR). POM has several features which demonstrate its scope and capability of performance optimization. First, most HLS tools depend exclusively on a single-level IR like LLVM IR to perform all the optimizations, introducing excessive information into the IR and making debugging an arduous task. In contrast, POM explicitly introduces three layers of IR to perform operations at suitable abstraction levels, streamlining the implementation and debugging process and exhibiting better flexibility, extensibility, and systematicness. Second, POM integrates the polyhedral model into MLIR and hence enables advanced dependence analysis and a wide range of FPGA-oriented loop transformations. By representing nested loops with integer sets and maps at suitable IR, loop transformations can be conducted conveniently through a series of manipulations on polyhedral semantics. Finally, to further relieve design effort, POM is equipped with a user-friendly programming interface (DSL) that allows a concise description of computation and includes a rich collection of scheduling primitives. An automatic design space exploration (DSE) engine is also provided to search for high-performance optimization schemes efficiently and generate optimized accelerators automatically. Experimental results show that POM achieves a 6.46× average speedup on typical benchmark suites and a 6.06 ×average speedup on real-world applications compared to the state-of-the-art.},
  keywords={Productivity;Processor scheduling;Semantics;Debugging;Programming;Extensibility;Space exploration},
  doi={10.1109/HPCA57654.2024.00017},
  ISSN={2378-203X},
  month={March},
  link={http://192.168.0.2:5012/2024/01/10/21a535013ed2f7168c4b81c9a6addfbbc9244e5f/}
}

@article{ppcg,
  author = {Verdoolaege, Sven and Carlos Juega, Juan and Cohen, Albert and Ignacio G\'{o}mez, Jos\'{e} and Tenllado, Christian and Catthoor, Francky},
  title = {Polyhedral parallel code generation for CUDA},
  year = {2013},
  issue_date = {January 2013},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {9},
  number = {4},
  issn = {1544-3566},
  doi = {10.1145/2400682.2400713},
  abstract = {This article addresses the compilation of a sequential program for parallel execution on a modern GPU. To this end, we present a novel source-to-source compiler called PPCG. PPCG singles out for its ability to accelerate computations from any static control loop nest, generating multiple CUDA kernels when necessary. We introduce a multilevel tiling strategy and a code generation scheme for the parallelization and locality optimization of imperfectly nested loops, managing memory and exposing concurrency according to the constraints of modern GPUs. We evaluate our algorithms and tool on the entire PolyBench suite.},
  journal = {ACM Trans. Archit. Code Optim.},
  month = {jan},
  articleno = {54},
  numpages = {23},
  keywords = {loop transformations, compilers, code generation, Polyhedral model, Par4All, PPCG., GPU, CUDA, C-to-CUDA},
}

@INPROCEEDINGS{proteus,
  author={Bambhaniya, Abhimanyu Rajeshkumar and Chen, Yangyu and Anshuman and Banerjee, Rohan and Krishna, Tushar},
  booktitle={2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)}, 
  title={Proteus : HLS-based NoC Generator and Simulator}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Networks-on-chip (NoCs) form the backbone fabric for connecting multi-core SoCs containing several processor cores and memories. Design-space exploration (DSE) of NoCs is a crucial part of the SoC design process to ensure that it does not become a bottleneck. DSE today is often hindered by the inherent trade-off between software simulation vs hardware emulation/e- valuation. Software simulators are easily extendable and allow for the evaluation of new ideas but are not able to capture the hardware complexity. Meanwhile, RTL development is known to be time-consuming. This has forced DSE to use simulators followed by RTL development, evaluation and feedback, which slows down the overall design process. In an effort to tackle this problem, we present Proteus, a configurable and modular NoC simulator and RTL generator. Proteus is the first of its kind framework to use HLS compiler to develop NoCs from a C++ description of the N oC circuit. These generated N oCs can be simulated in software and tested on FPGAs. This allows users to do rapid DSE by providing the opportunity to tweak and test NoC architectures in real-time. We also compare Proteus-generated RTL with Chisel- generated and hand-written RTL in terms of area, timing and productivity. The ability to synthesize the NoC design on FPGAs can benefit large designs as the custom hardware results in faster run-time than cycle-accurate software simulators. Proteus is modeled similar to existing state-of-the-art simulators and offers users modifiable parameters to generate custom topologies, routing algorithms, and router microarchitectures.},
  keywords={Software algorithms;C++ languages;Routing;Software;Hardware;Generators;Topology;NoC Generator;Vitis HLS;Chisel;FPGA;NoC Simulator},
  doi={10.23919/DATE56975.2023.10137173},
  ISSN={1558-1101},
  month={April},
}
@INPROCEEDINGS{rubick_dac,
  author={Luo, Zizhang and Lu, Liqiang and Zheng, Size and Yin, Jieming and Cong, Jason and Yin, Jianwei and Liang, Yun},
  booktitle={2023 60th ACM/IEEE Design Automation Conference (DAC)}, 
  title={Rubick: A Synthesis Framework for Spatial Architectures via Dataflow Decomposition}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Dataflows are critical for spatial architectures designed for tensor applications. Prior works develop various notations and hardware generation frameworks for dataflows. However, due to the semantic gap between notations and low-level details, analysis based on these notations cannot capture the detailed architectural features between different dataflows, so these works failed to provide architectural optimization and efficient design space exploration (DSE) at the same time.We propose Rubick, a synthesis framework for spatial architecture. Rubick decomposes the dataflow into two low-level intermediate representations including access entry and data layout. Access entry specifies how data enter into the PE arrays from memory, while data layout specifies how data are arranged and accessed. Based on this decomposition, Rubick provides efficient DSE and generates optimized hardware. Experiments show that the DSE time is accelerated by up to 1.1×105X and performance on FPGA is improved by 13%.},
  keywords={Tensors;Design automation;Layout;Semantics;Hardware;Spatial databases;Space exploration;Dataflow architectures;Automatic synthesis},
  doi={10.1109/DAC56929.2023.10247743},
  ISSN={},
  month={July},
  link={http://192.168.0.2:5012/2023/07/09/9e55e8432511db56dac862e0fffdbd9ea2c08e3d/},
}

@ARTICLE{rubick_tcad,
  author={Lu, Liqiang and Luo, Zizhang and Zheng, Size and Yin, Jieming and Cong, Jason and Liang, Yun and Yin, Jianwei},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={Rubick: A Unified Infrastructure for Analyzing, Exploring, and Implementing Spatial Architectures via Dataflow Decomposition}, 
  year={2024},
  volume={43},
  number={4},
  pages={1177-1190},
  abstract={The fast-growing tensor applications expose tremendous dataflow alternatives when implemented on spatial architectures that feature large PE arrays and abundant interconnection resources. Prior works develop various notations and performance models for dataflows. Though these notations are very useful for understanding the reuse, bandwidth, and performance of dataflows, they do not define the underlying hardware implementation. Due to the semantic gap, analysis based on these notations cannot capture the detailed architectural features between different dataflows, leading to inefficient design space exploration and suboptimal designs. To address these issues, we propose Rubick, a unified infrastructure for analyzing, exploring, and implementing spatial architectures. The main innovation of Rubick is it decomposes the dataflow into two low-level intermediate representations: 1) access entry and 2) data layout. Access entry specifies how data enter into the PE arrays from memory, while data layout specifies how data are arranged and accessed. These two representations allow us to infer the hardware implementation details, such as PE interconnection and memory structure, which are amenable for structural analysis and systematic exploration. Based on this decomposition analysis, Rubick provides opportunities for micro-architecture optimization and efficient design space exploration. Our experiments demonstrate that Rubick can reduce 82.4% of wire resources with only a 2.7% latency increase by optimizing access entry IR, and achieve 70.8% memory overhead reduction by optimizing data layout IR. Rubick also accelerates the DSE time of dataflows by up to  $1.1\times 10^{5}\text{X}$ , saving the time from several days to minutes. The source code of Rubick is publically available on (https://link-omitted-for-blind-review).},
  keywords={Tensors;Computer architecture;Hardware;Layout;Spatial databases;Space exploration;Semantics;Accelerator architectures;AI accelerators;performance analysis},
  doi={10.1109/TCAD.2023.3337208},
  ISSN={1937-4151},
  month={April},
  link={http://192.168.0.2:5012/2023/11/28/f5ee953fba3959b63db569ae03432f7c5e8aeb44/},
}

@INPROCEEDINGS{scale-sim,
  author={Samajdar, Ananda and Joseph, Jan Moritz and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
  booktitle={2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim}, 
  year={2020},
  volume={},
  number={},
  pages={58-68},
  abstract={The compute demand for deep learning workloads is well known and is a prime motivator for powerful parallel computing platforms such as GPUs or dedicated hardware accelerators. The massive inherent parallelism of these workloads enables us to extract more performance by simply provisioning more compute hardware for a given task. This strategy can be directly exploited to build higher-performing hardware for DNN workloads, by incorporating as many parallel compute units as possible in a single system. This strategy is referred to as scaling up. Alternatively, it's feasible to arrange multiple hardware systems to work on a single problem, and in some cases, a cheaper alternative to exploit the given parallelism, or in other words, scaling out. As DNN based solutions become increasingly prevalent, so does the demand for computation, making the scaling choice (scale-up vs scale-out) critical. To study this design-space, this work makes two major contributions. (i) We describe a cycle-accurate simulator called SCALE-SIM for DNN inference on systolic arrays, which we use to model both scale-up and scale-out systems, modeling on-chip memory access, runtime, and DRAM bandwidth requirements for a given workload. (ii) We also present an analytical model to estimate the optimal scale-up vs scale-out ratio given hardware constraints (e.g, TOPS and DRAM bandwidth) for a given workload. We observe that a judicious choice of scaling can lead to performance improvements as high as 50 per layer, within the available DRAM bandwidth. This work demonstrates and analyzes the trade-off space for performance, DRAM bandwidth, and energy, and identifies sweet spots for various workloads and hardware configurations.},
  keywords={Analytical models;Systematics;Memory management;Random access memory;Bandwidth;Parallel processing;Systems modeling;Accelerator Simulator;DNN acclerator;Cycle accurate simulation;scaling analysis},
  doi={10.1109/ISPASS48437.2020.00016},
  ISSN={},
  month={Aug},
  link={http://192.168.0.2:5012/2020/08/01/4d5254407ec01e6c151fb4f102547ff10fe6c9ed/},
}

@inproceedings{scnn,
author = {Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen W. and Dally, William J.},
title = {SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3079856.3080254},
abstract = {Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to a multiplier array, where they are extensively reused; product accumulation is performed in a novel accumulator array. On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7x and 2.3x, respectively, over a comparably provisioned dense CNN accelerator.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {27–40},
numpages = {14},
keywords = {accelerator architecture, Convolutional neural networks},
location = {Toronto, ON, Canada},
series = {ISCA '17},
link = {http://192.168.0.2:5012/2017/05/23/402f850dff86fb601d34b2841e6083ac0f928edd/},
}
@INPROCEEDINGS{shidiannao,
  author={Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},
  booktitle={2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)}, 
  title={ShiDianNao: Shifting vision processing closer to the sensor}, 
  year={2015},
  volume={},
  number={},
  pages={92-104},
  abstract={In recent years, neural network accelerators have been shown to achieve both high energy efficiency and high performance for a broad application scope within the important category of recognition and mining applications. Still, both the energy efficiency and peiformance of such accelerators remain limited by memory accesses. In this paper, we focus on image applications, arguably the most important category among recognition and mining applications. The neural networks which are state-of-the-art for these applications are Convolutional Neural Networks (CNN), and they have an important property: weights are shared among many neurons, considerably reducing the neural network memory footprint. This property allows to entirely map a CNN within an SRAM, eliminating all DRAM accesses for weights. By further hoisting this accelerator next to the image sensor, it is possible to eliminate all remaining DRAM accesses, i.e., for inputs and outputs. In this paper, we propose such a CNN accelerator, placed next to a CMOS or CCD sensor. The absence of DRAM accesses combined with a careful exploitation of the specific data access patterns within CNNs allows us to design an accelerator which is 60x more energy efficient than the previous state-of-the-art neural network accelerator. We present a fult design down to the layout at 65 nm, with a modest footprint of 4.86 mm2 and consuming only 320 mW, but still about 30x faster than high-end GPUs.},
  keywords={Kernel;Neural networks;Sensors;Energy efficiency;Smart phones;Filtering;Neurons},
  doi={10.1145/2749469.2750389},
  ISSN={1063-6897},
  month={June},
}
@INPROCEEDINGS{shortcut-mining,
  author={Azizimazreah, Arash and Chen, Lizhong},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Shortcut Mining: Exploiting Cross-Layer Shortcut Reuse in DCNN Accelerators}, 
  year={2019},
  volume={},
  number={},
  pages={94-105},
  abstract={Off-chip memory traffic has been a major performance bottleneck in deep learning accelerators. While reusing on-chip data is a promising way to reduce off-chip traffic, the opportunity on reusing shortcut connection data in deep networks (e.g., residual networks) have been largely neglected. Those shortcut data accounts for nearly 40% of the total feature map data. In this paper, we propose Shortcut Mining, a novel approach that “mines” the unexploited opportunity of on-chip data reusing. We introduce the abstraction of logical buffers to address the lack of flexibility in existing buffer architecture, and then propose a sequence of procedures which, collectively, can effectively reuse both shortcut and non-shortcut feature maps. The proposed procedures are also able to reuse shortcut data across any number of intermediate layers without using additional buffer resources. Experiment results from prototyping on FPGAs show that, the proposed Shortcut Mining achieves 53.3%, 58%, and 43% reduction in off-chip feature map traffic for SqueezeNet, ResNet-34, and ResNet152, respectively and a 1.93X increase in throughput compared with a state-of-the-art accelerator.},
  keywords={System-on-chip;Field programmable gate arrays;Tin;Deep learning;Memory management;Data models;deep learning;Convolutional Neural Network;Hardware Accelerator},
  doi={10.1109/HPCA.2019.00030},
  ISSN={2378-203X},
  month={Feb},
  link={http://192.168.0.2:5012/2019/02/01/1e3d214c12f4fc9793b8cac32751a684553f5b3a/},
}
@INPROCEEDINGS{sigma,
  author={Qin, Eric and Samajdar, Ananda and Kwon, Hyoukjun and Nadella, Vineet and Srinivasan, Sudarshan and Das, Dipankar and Kaul, Bharat and Krishna, Tushar},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training}, 
  year={2020},
  volume={},
  number={},
  pages={58-70},
  abstract={The advent of Deep Learning (DL) has radically transformed the computing industry across the entire spectrum from algorithms to circuits. As myriad application domains embrace DL, it has become synonymous with a genre of workloads across vision, speech, language, recommendations, robotics, and games. The key compute kernel within most DL workloads is general matrix-matrix multiplications (GEMMs), which appears frequently during both the forward pass (inference and training) and backward pass (training). GEMMs are a natural choice for hardware acceleration to speed up training, and have led to 2D systolic architectures like NVIDIA tensor cores and Google Tensor Processing Unit (TPU). Unfortunately, emerging GEMMs in DL are highly irregular and sparse, which lead to poor data mappings on systolic architectures. This paper proposes SIGMA, a flexible and scalable architecture that offers high utilization of all its processing elements (PEs) regardless of kernel shape and sparsity. Within SIGMA includes a novel reduction tree microarchitecture named Forwarding Adder Network (FAN). SIGMA performs 5.7x better than systolic array architectures for irregular sparse matrices, and roughly 3x better than state-of-the-art sparse accelerators. We demonstrate an instance of SIGMA operating at 10.8 TFLOPS efficiency across arbitrary levels of sparsity, with a 65.10 mm^2 and 22.33 W footprint on a 28 nm process.},
  keywords={Training;Sparse matrices;Arrays;Graphics processing units;Engines;Kernel;Tensile stress},
  doi={10.1109/HPCA47549.2020.00015},
  ISSN={2378-203X},
  month={Feb},
  link={http://192.168.0.2:5012/2020/02/01/fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345/},
}
@ARTICLE{simba_jscc,
  author={Zimmer, Brian and Venkatesan, Rangharajan and Shao, Yakun Sophia and Clemons, Jason and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel S. and Gray, C. Thomas and Keckler, Stephen W. and Khailany, Brucek},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={A 0.32–128 TOPS, Scalable Multi-Chip-Module-Based Deep Neural Network Inference Accelerator With Ground-Referenced Signaling in 16 nm}, 
  year={2020},
  volume={55},
  number={4},
  pages={920-932},
  abstract={Custom accelerators improve the energy efficiency, area efficiency, and performance of deep neural network (DNN) inference. This article presents a scalable DNN accelerator consisting of 36 chips connected in a mesh network on a multi-chip-module (MCM) using ground-referenced signaling (GRS). While previous accelerators fabricated on a single monolithic chip are optimal for specific network sizes, the proposed architecture enables flexible scaling for efficient inference on a wide range of DNNs, from mobile to data center domains. Communication energy is minimized with large on-chip distributed weight storage and a hierarchical network-on-chip and network-on-package, and inference energy is minimized through extensive data reuse. The 16-nm prototype achieves 1.29-TOPS/mm2 area efficiency, 0.11 pJ/op (9.5 TOPS/W) energy efficiency, 4.01-TOPS peak performance for a one-chip system, and 127.8 peak TOPS and 1903 images/s ResNet-50 batch-1 inference for a 36-chip system.},
  keywords={Tensors;Neural networks;Computer architecture;System-on-chip;Prototypes;Convolution;Bandwidth;Deep neural networks (DNNs);ground-referenced signaling (GRS);inference accelerator;multi-chip modules;single-ended signaling},
  doi={10.1109/JSSC.2019.2960488},
  ISSN={1558-173X},
  month={April},
  link={http://192.168.0.2:5012/2020/01/14/fe261bfd9d74f97268b3a9d9b700fdf5340ef434/},
}
@inproceedings{simba_micro,
  author = {Shao, Yakun Sophia and Clemons, Jason and Venkatesan, Rangharajan and Zimmer, Brian and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel and Gray, C. Thomas and Khailany, Brucek and Keckler, Stephen W.},
  title = {Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture},
  year = {2019},
  isbn = {9781450369381},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3352460.3358302},
  abstract = {Package-level integration using multi-chip-modules (MCMs) is a promising approach for building large-scale systems. Compared to a large monolithic die, an MCM combines many smaller chiplets into a larger system, substantially reducing fabrication and design costs. Current MCMs typically only contain a handful of coarse-grained large chiplets due to the high area, performance, and energy overheads associated with inter-chiplet communication. This work investigates and quantifies the costs and benefits of using MCMs with fine-grained chiplets for deep learning inference, an application area with large compute and on-chip storage requirements. To evaluate the approach, we architected, implemented, fabricated, and tested Simba, a 36-chiplet prototype MCM system for deep-learning inference. Each chiplet achieves 4 TOPS peak performance, and the 36-chiplet MCM package achieves up to 128 TOPS and up to 6.1 TOPS/W. The MCM is configurable to support a flexible mapping of DNN layers to the distributed compute and storage units. To mitigate inter-chiplet communication overheads, we introduce three tiling optimizations that improve data locality. These optimizations achieve up to 16\% speedup compared to the baseline layer mapping. Our evaluation shows that Simba can process 1988 images/s running ResNet-50 with batch size of one, delivering inference latency of 0.50 ms.},
  booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages = {14–27},
  numpages = {14},
  keywords = {Multi-chip module, accelerator architecture, neural networks},
  location = {Columbus, OH, USA},
  series = {MICRO '52},
  link = {http://192.168.0.2:5012/2019/10/12/8efd76fafff43a1b420f8cda092a8bab8d545732/},
}
@INPROCEEDINGS{simba_vlsi,
  author={Zimmer, Brian and Venkatesan, Rangharajan and Shao, Yakun Sophia and Clemons, Jason and Fojtik, Matthew and Jiang, Nan and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel and Raina, Priyanka and Tell, Stephen G. and Zhang, Yanqing and Dally, William J. and Emer, Joel S. and Gray, C. Thomas and Keckler, Stephen W. and Khailany, Brucek},
  booktitle={2019 Symposium on VLSI Circuits}, 
  title={A 0.11 pJ/Op, 0.32-128 TOPS, Scalable Multi-Chip-Module-based Deep Neural Network Accelerator with Ground-Reference Signaling in 16nm}, 
  year={2019},
  volume={},
  number={},
  pages={C300-C301},
  abstract={This work presents a scalable deep neural network (DNN) accelerator consisting of 36 chips connected in a mesh network on a multi-chip-module (MCM) using ground-referenced signaling (GRS). While previous accelerators fabricated on a single monolithic die are limited to specific network sizes, the proposed architecture enables flexible scaling for efficient inference on a wide range of DNNs, from mobile to data center domains. The 16nm prototype achieves 1.29 TOPS/mm2, 0.11 pJ/op energy efficiency, 4.01 TOPS peak performance for a 1chip system, and 127.8 peak TOPS and 2615 images/s ResNet50 inference for a 36-chip system.},
  keywords={Transceivers;Random access memory;Energy efficiency;Neural networks;Bandwidth;Semiconductor device measurement;Weight measurement},
  doi={10.23919/VLSIC.2019.8778056},
  ISSN={2158-5636},
  month={June},
  link={http://192.168.0.2:5012/2019/06/01/d869b5264eb1526c98ea5e1942b6e25ad47c2bfb/},
}
@inproceedings{sparten,
author = {Gondimalla, Ashish and Chesnut, Noah and Thottethodi, Mithuna and Vijaykumar, T. N.},
title = {SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3352460.3358291},
abstract = {Convolutional neural networks (CNNs) are emerging as powerful tools for image processing. Recent machine learning work has reduced CNNs' compute and data volumes by exploiting the naturally-occurring and actively-transformed zeros in the feature maps and filters. While previous semi-sparse architectures exploit one-sided sparsity either in the feature maps or the filters, but not both, a recent fully-sparse architecture, called Sparse CNN (SCNN), exploits two-sided sparsity to improve performance and energy over dense architectures. However, sparse vector-vector dot product, a key primitive in sparse CNNs, would be inefficient using the representation adopted by SCNN. The dot product requires finding and accessing non-zero elements in matching positions in the two sparse vectors -- an inner join using the position as the key with a single value field. SCNN avoids the inner join by performing a Cartesian product capturing the relevant multiplications. However, SCNN's approach incurs several considerable overheads and is not applicable to non-unit-stride convolutions. Further, exploiting reuse in sparse CNNs fundamentally causes systematic load imbalance not addressed by SCNN. We propose SparTen which achieves efficient inner join by providing support for native two-sided sparse execution and memory storage. To tackle load imbalance, SparTen employs a software scheme, called greedy balancing, which groups filters by density via two variants, a software-only one which uses whole-filter density and a software-hardware hybrid which uses finer-grain density. Our simulations show that, on average, SparTen performs 4.7x, 1.8x, and 3x better than a dense architecture, one-sided sparse architecture, and SCNN, respectively. An FPGA implementation shows that SparTen performs 4.3x and 1.9x better than a dense architecture and a one-sided sparse architecture, respectively.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {151–165},
numpages = {15},
keywords = {Accelerators, Convolutional neural networks, Sparse tensors},
location = {Columbus, OH, USA},
series = {MICRO '52},
link = {http://192.168.0.2:5012/2019/10/12/f9cd024497e1504c07a606a0df277441819a9e0d/}
}

@inproceedings{spatial,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: a language and compiler for application accelerators},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
link = {https://doi.org/10.1145/3192366.3192379},
doi = {10.1145/3192366.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult.  In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42\% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {296–311},
numpages = {16},
keywords = {CGRAs, FPGAs, compilers, domain-specific languages, hardware accelerators, high-level synthesis, reconfigurable architectures},
location = {Philadelphia, PA, USA},
series = {PLDI 2018},
link = {http://192.168.0.2:5012/2018/06/11/0553ac1fe3cff2e42110e02e0d86d48d71407eec/},
}

@ARTICLE{stonne,
  author={Muñoz-Martínez, Francisco and Abellán, José L. and Acacio, Manuel E. and Krishna, Tushar},
  journal={IEEE Computer Architecture Letters}, 
  title={STONNE: Enabling Cycle-Level Microarchitectural Simulation for DNN Inference Accelerators}, 
  year={2021},
  volume={20},
  number={2},
  pages={122-125},
  abstract={The design of specialized architectures for accelerating the inference procedure of Deep Neural Networks (DNNs) is a booming area of research nowadays. While first-generation rigid accelerator proposals used simple fixed dataflows tailored for dense DNNs, more recent architectures have argued for flexibility to efficiently support a wide variety of layer types, dimensions, and sparsity. As the complexity of these accelerators grows, the analytical models currently being used prove unable to capture execution-time subtleties, thus resulting inexact in many cases. We present STONNE (Simulation TOol of Neural Network Engines), a cycle-level microarchitectural simulator for state-of-the-art rigid and flexible DNN inference accelerators that can plug into any high-level DNN framework as an accelerator device, and perform full-model evaluation of both dense and sparse real, unmodified DNN models.},
  keywords={Analytical models;Accelerators;Microarchitecture;Bandwidth;Mathematical model;Deep learning;Computational modeling;Hardware simulation;DNN accelerators;specific hardware for deep learning;computer architecture ?>},
  doi={10.1109/LCA.2021.3097253},
  ISSN={1556-6064},
  month={July},
  link={http://192.168.0.2:5012/2021/07/01/3888d6786f8ce42fd64729e018f42fd372143048/},
}

@inproceedings{susy,
  author = {Lai, Yi-Hsiang and Rong, Hongbo and Zheng, Size and Zhang, Weihao and Cui, Xiuping and Jia, Yunshan and Wang, Jie and Sullivan, Brendan and Zhang, Zhiru and Liang, Yun and Zhang, Youhui and Cong, Jason and George, Nithin and Alvarez, Jose and Hughes, Christopher and Dubey, Pradeep},
  title = {SuSy: a programming model for productive construction of high-performance systolic arrays on FPGAs},
  year = {2020},
  isbn = {9781450380263},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  link = {https://doi.org/10.1145/3400302.3415644},
  doi = {10.1145/3400302.3415644},
  abstract = {Systolic algorithms are one of the killer applications on spatial architectures such as FPGAs and CGRAs. However, it requires a tremendous amount of human effort to design and implement a high-performance systolic array for a given algorithm using the traditional RTL-based methodology. On the other hand, existing high-level synthesis (HLS) tools either (1) force the programmers to do "micro-coding" where too many optimizations must be carried out through tedious code restructuring and insertion of vendor-specific pragmas, or (2) give them too little control to influence a push-button compilation flow to achieve high quality of results.To tackle these challenges, we introduce SuSy, a programming framework composed of a domain-specific language (DSL) and a compilation flow that enables programmers to productively build high-performance systolic arrays on FPGAs. With SuSy, programmers express the design functionality in the form of uniform recurrence equations (UREs), which can describe algorithms from a wide spectrum of applications as long as the underlying computation has a uniform dependence structure. The URE description in SuSy is followed by a set of decoupled spatial mapping primitives that specify how to map the equations to a spatial architecture. More concretely, programmers can apply space-time transformations and several other memory and I/O optimizations to build a highly efficient systolic architecture productively. Experimental results show that SuSy can describe various algorithms with UREs and generate high-performance systolic arrays by spatial optimizations. For instance, the SGEMM benchmark written in SuSy can approach the performance of the manual design optimized by experts, while using 30\texttimes{} fewer lines of code.},
  booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
  articleno = {73},
  numpages = {9},
  keywords = {DSL, FPGA, URE, space-time transformation, systolic array},
  location = {Virtual Event, USA},
  series = {ICCAD '20},
  link = {http://192.168.0.2:5012/2020/11/02/38a8bfec87d6cc1eea2d4fe77370f846d2d37376/},
}
@INPROCEEDINGS{t2s-tensor,
  author={Srivastava, Nitish and Rong, Hongbo and Barua, Prithayan and Feng, Guanyu and Cao, Huanqi and Zhang, Zhiru and Albonesi, David and Sarkar, Vivek and Chen, Wenguang and Petersen, Paul and Lowney, Geoff and Herr, Adam and Hughes, Christopher and Mattson, Timothy and Dubey, Pradeep},
  booktitle={2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)}, 
  title={T2S-Tensor: Productively Generating High-Performance Spatial Hardware for Dense Tensor Computations}, 
  year={2019},
  volume={},
  number={},
  pages={181-189},
  keywords={Optimization;Computer architecture;Kernel;Field programmable gate arrays;Programming;Hardware;Compiler;Domain Specific Language;High Level Synthesis;Spatial Computing;FPGA;CGRA},
  doi={10.1109/FCCM.2019.00033},
  link={http://192.168.0.2:5012/2019/04/01/2b98e73641437e8a5bd67fb2a72318caea841a2a/},
}

@INPROCEEDINGS{tenet,
  author={Lu, Liqiang and Guan, Naiqing and Wang, Yuyue and Jia, Liancheng and Luo, Zizhang and Yin, Jieming and Cong, Jason and Liang, Yun},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation}, 
  year={2021},
  volume={},
  number={},
  pages={720-733},
  abstract={Accelerating tensor applications on spatial architectures provides high performance and energy-efficiency, but requires accurate performance models for evaluating various dataflow alternatives. Such modeling relies on the notation of tensor dataflow and the formulation of performance metrics. Recent proposed compute-centric and data-centric notations describe the dataflow using imperative directives. However, these two notations are less expressive and thus lead to limited optimization opportunities and inaccurate performance models.In this paper, we propose a framework TENET that models hardware dataflow of tensor applications. We start by introducing a relation-centric notation, which formally describes the hardware dataflow for tensor computation. The relation-centric notation specifies the hardware dataflow, PE interconnection, and data assignment in a uniform manner using relations. The relation-centric notation is more expressive than the compute-centric and data-centric notations by using more sophisticated affine transformations. Another advantage of relation-centric notation is that it inherently supports accurate metrics estimation, including data reuse, bandwidth, latency, and energy. TENET computes each performance metric by counting the relations using integer set structures and operators. Overall, TENET achieves 37.4% and 51.4% latency reduction for CONV and GEMM kernels compared with the state-of-the-art data-centric notation by identifying more sophisticated hardware dataflows.},
  keywords={Measurement;Tensors;Computational modeling;Estimation;Computer architecture;Bandwidth;Benchmark testing;Relation;Dataflow;Tensor;Performance model;Spatial Architecture},
  doi={10.1109/ISCA52012.2021.00062},
  ISSN={2575-713X},
  month={June},
  link={http://192.168.0.2:5012/2021/05/05/ae464dc54a594de682fddd59479736b1e65bbf52/},
}

@article{tensor_comprehensions,
  title={Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions},
  author={Nicolas Vasilache and Oleksandr Zinenko and Theodoros Theodoridis and Priya Goyal and Zach DeVito and William S. Moses and Sven Verdoolaege and Andrew Adams and Albert Cohen},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.04730},
  link={https://api.semanticscholar.org/CorpusID:44014988}
}
@INPROCEEDINGS{tensorlib_dac,
  author={Jia, Liancheng and Luo, Zizhang and Lu, Liqiang and Liang, Yun},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)}, 
  title={TensorLib: A Spatial Accelerator Generation Framework for Tensor Algebra}, 
  year={2021},
  volume={},
  number={},
  pages={865-870},
  abstract={Tensor algebra finds applications in various domains, and these applications, especially when accelerated on spatial hardware accelerators, can deliver high performance and low power. Spatial hardware accelerator exhibits complex design space. Prior approaches based on manual implementation lead to low programming productivity, rendering thorough design space exploration impossible. In this paper, we propose TensorLib, a framework for generating spatial hardware accelerator for tensor algebra applications. TensorLib is motivated by the observation that, different dataflows share common hardware modules, which can be reused across different designs. To build such a framework, TensorLib first uses Space-Time Transformation to explore different dataflows, which can compactly represent the hardware dataflow using a simple transformation matrix. Next, we identify the common structures of different dataflows and build parameterized hardware module templates with Chisel. Our generation framework can select the needed hardware modules for each dataflow, connect the modules using a specified interconnection pattern, and automatically generate the complete hardware accelerator design. TensorLib remarkably improves the productivity for the development and optimization of spatial hardware architecture, providing a rich design space with tradeoffs in performance, area, and power. Experiments show that TensorLib can automatically generate hardware designs with different dataflows and achieve 21% performance improvement on FPGA compared to the state-of-the-arts.},
  keywords={Productivity;Tensors;Algebra;Programming;Rendering (computer graphics);System-on-chip;Space exploration},
  doi={10.1109/DAC18074.2021.9586329},
  ISSN={0738-100X},
  month={Dec},
  link={http://192.168.0.2:5012/2021/04/26/e370ac1adb6d676aeceb2012f4cf2e751e17e377/}
}
@ARTICLE{tensorlib_tcad,
  author={Jia, Liancheng and Luo, Zizhang and Lu, Liqiang and Liang, Yun},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={Automatic Generation of Spatial Accelerator for Tensor Algebra}, 
  year={2023},
  volume={42},
  number={6},
  pages={1898-1911},
  abstract={Tensor algebra finds applications in various domains, including machine learning applications, data analytics, and others. Spatial hardware accelerators are widely used to boost the performance of tensor algebra applications. It has a complex hardware architecture and rich design space. Prior approaches based on manual implementation lead to low programming productivity, making it hard to explore the large design space. In this article, we propose Tensorlib, a framework for generating spatial hardware accelerators for tensor algebra applications. Tensorlib is motivated by the observation that, tensor dataflows can be expressed with linear transformations, and they share common hardware modules which can be reused across different designs. Tensorlib first uses space-time transformation to explore different dataflows, which can compactly represent the hardware dataflow using a transformation matrix. Next, we identify the common structures of different dataflows and build parameterized hardware module templates. Our generation framework can select the needed hardware modules for each dataflow, connect the modules using a specified interconnection pattern, and automatically generate the complete hardware accelerator design. Tensorlib remarkably improves the productivity for the development and optimization of spatial hardware architecture, providing a rich design space with tradeoffs in performance, area, and power. Experiments show that Tensorlib can automatically generate hardware designs with different dataflows for a variety of tensor algebra programs. Tensorlib can achieve 318-MHz frequency and 786-GFLOP/s throughput for matrix multiplication kernel on Xilinx VU9P FPGA, which outperforms the state-of-the-art generators.},
  keywords={Hardware;Tensors;System-on-chip;Algebra;Computer architecture;Convolution;Codes;Accelerator architectures;linear algebra;parallel architectures;programmable logic arrays},
  doi={10.1109/TCAD.2022.3209949},
  ISSN={1937-4151},
  month={June},
  link={http://192.168.0.2:5012/2023/06/01/6f277291e381c01bccd0f597a249af6d76379483/},
}

@INPROCEEDINGS{timeloop,
  author={Parashar, Angshuman and Raina, Priyanka and Shao, Yakun Sophia and Chen, Yu-Hsin and Ying, Victor A. and Mukkara, Anurag and Venkatesan, Rangharajan and Khailany, Brucek and Keckler, Stephen W. and Emer, Joel},
  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation}, 
  year={2019},
  volume={},
  number={},
  pages={304-315},
  abstract={This paper presents Timeloop, an infrastructure for evaluating and exploring the architecture design space of deep neural network (DNN) accelerators. Timeloop uses a concise and unified representation of the key architecture and implementation attributes of DNN accelerators to describe a broad space of hardware topologies. It can then emulate those topologies to generate an accurate projection of performance and energy efficiency for a DNN workload through a mapper that finds the best way to schedule operations and stage data on the specified architecture. This enables fair comparisons across different architectures and makes DNN accelerator design more systematic. This paper describes Timeloop's underlying models and algorithms in detail and shows results from case studies enabled by Timeloop, which provide interesting insights into the current state of DNN architecture design. In particular, they reveal that dataflow and memory hierarchy co-design plays a critical role in optimizing energy efficiency. Also, there is currently still not a single architecture that achieves the best performance and energy efficiency across a diverse set of workloads due to flexibility and efficiency trade-offs. These results provide inspiration into possible directions for DNN accelerator research.},
  keywords={Hardware;Neural networks;Space exploration;Systematics;Accelerator architectures;Computational modeling;modeling;accelerator architecture;deep neural networks;neural network dataflows},
  doi={10.1109/ISPASS.2019.00042},
  ISSN={},
  month={March},
  link={http://192.168.0.2:5012/2019/03/01/9ded246119861dd325e7004b2050bba310e08797/}
}

@INPROCEEDINGS{tiramisu,
  author={Baghdadi, Riyadh and Ray, Jessica and Romdhane, Malek Ben and Sozzo, Emanuele Del and Akkas, Abdurrahman and Zhang, Yunming and Suriana, Patricia and Kamil, Shoaib and Amarasinghe, Saman},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code}, 
  year={2019},
  volume={},
  number={},
  pages={193-205},
  abstract={This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel commands to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.},
  keywords={Processor scheduling;Optimization;Pluto;DSL;Multicore processing;Layout;Code Optimization;Code Generation;Polyhedral Model;Deep Learning;Tensors;GPUs;Distributed Systems},
  doi={10.1109/CGO.2019.8661197},
  ISSN={},
  month={Feb},
}

@INPROCEEDINGS{tpu,
  author={Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={In-datacenter performance analysis of a tensor processing unit}, 
  year={2017},
  volume={},
  number={},
  pages={1-12},
  abstract={Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X–30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X–80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
  keywords={Transmission line matrix methods;Graphics processing units;Artificial neural networks;Central Processing Unit;Tensile stress;Training;Hardware;DNN;MLP;CNN;RNN;LSTM;neural network;deep learning;domain-specific architecture;accelerator;TensorFlow;TPU;GPU},
  doi={10.1145/3079856.3080246},
  ISSN={},
  month={June},
  link={http://192.168.0.2:5012/2017/04/16/2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22/}
}
@inproceedings{triggered_instructions,
  author = {Parashar, Angshuman and Pellauer, Michael and Adler, Michael and Ahsan, Bushra and Crago, Neal and Lustig, Daniel and Pavlov, Vladimir and Zhai, Antonia and Gambhir, Mohit and Jaleel, Aamer and Allmon, Randy and Rayess, Rachid and Maresh, Stephen and Emer, Joel},
  title = {Triggered instructions: a control paradigm for spatially-programmed architectures},
  year = {2013},
  isbn = {9781450320795},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2485922.2485935},
  abstract = {In this paper, we present triggered instructions, a novel control paradigm for arrays of processing elements (PEs) aimed at exploiting spatial parallelism. Triggered instructions completely eliminate the program counter and allow programs to transition concisely between states without explicit branch instructions. They also allow efficient reactivity to inter-PE communication traffic. The approach provides a unified mechanism to avoid over-serialized execution, essentially achieving the effect of techniques such as dynamic instruction reordering and multithreading, which each require distinct hardware mechanisms in a traditional sequential architecture.Our analysis shows that a triggered-instruction based spatial accelerator can achieve 8X greater area-normalized performance than a traditional general-purpose processor. Further analysis shows that triggered control reduces the number of static and dynamic instructions in the critical paths by 62\% and 64\% respectively over a program-counter style spatial baseline, resulting in a speedup of 2.0X.},
  booktitle = {Proceedings of the 40th Annual International Symposium on Computer Architecture},
  pages = {142–153},
  numpages = {12},
  keywords = {reconfigurable accelerators, spatial programming},
  location = {Tel-Aviv, Israel},
  series = {ISCA '13},
  link = {http://192.168.0.2:5012/2013/06/23/91ac70c712b1fce4836ffccb25b3ff6c7de0adfd/},
}

@inproceedings{triton,
author = {Tillet, Philippe and Kung, H. T. and Cox, David},
title = {Triton: an intermediate language and compiler for tiled neural network computations},
year = {2019},
isbn = {9781450367196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
link = {https://doi.org/10.1145/3315508.3329973},
doi = {10.1145/3315508.3329973},
abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.},
booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {10–19},
numpages = {10},
keywords = {GPU, compiler, neural networks},
location = {Phoenix, AZ, USA},
series = {MAPL 2019}
}
@inproceedings{tvm,
  title={TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  author={Tianqi Chen and Thierry Moreau and Ziheng Jiang and Haichen Shen and Eddie Q. Yan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
  year={2018},
  link={https://api.semanticscholar.org/CorpusID:3296374}
}
@INPROCEEDINGS{union,
  author={Jeong, Geonhwa and Kestor, Gokcen and Chatarasi, Prasanth and Parashar, Angshuman and Tsai, Po-An and Rajamanickam, Sivasankaran and Gioiosa, Roberto and Krishna, Tushar},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)}, 
  title={Union: A Unified HW-SW Co-Design Ecosystem in MLIR for Evaluating Tensor Operations on Spatial Accelerators}, 
  year={2021},
  volume={},
  number={},
  pages={30-44},
  abstract={To meet the extreme compute demands for deep learning across commercial and scientific applications, dataflow accelerators are becoming increasingly popular. While these “domain-specific” accelerators are not fully programmable like CPUs and GPUs, they retain varying levels of flexibility with respect to data orchestration, i.e., dataflow and tiling optimizations to enhance efficiency. There are several challenges when designing new algorithms and mapping approaches to execute the algorithms for a target problem on new hardware. Previous works have addressed these challenges individually. To address this challenge as a whole, in this work, we present a HW-SW codesign ecosystem for spatial accelerators called Union11https://github.com/union-codesign/union within the popular MLIR compiler infrastructure. Our framework allows exploring different algorithms and their mappings on several accelerator cost models. Union also includes a plug-and-play library of accelerator cost models and mappers which can easily be extended. The algorithms and accelerator cost models are connected via a novel mapping abstraction that captures the map space of spatial accelerators which can be systematically pruned based on constraints from the hardware, workload, and mapper. We demonstrate the value of Union for the community with several case studies which examine offloading different tensor operations (CONV/GEMM/Tensor Contraction) on diverse accelerator architectures using different mapping schemes.},
  keywords={Tensors;Costs;Biological system modeling;Ecosystems;Hardware;Software;Libraries;Spatial accelerators;MLIR;Deep learning},
  doi={10.1109/PACT52795.2021.00010},
  ISSN={},
  month={Sep.},
  link={http://192.168.0.2:5012/2021/09/01/24714752eb2a606e6097994bf09c0f3a5cde6031/}
}

@inproceedings{vasilache,
  title={Joint Scheduling and Layout Optimization to Enable Multi-Level Vectorization},
  author={Nicolas Vasilache and Beno{\^i}t Meister and Muthu Manikandan Baskaran and Richard A. Lethin},
  year={2012},
  link={https://api.semanticscholar.org/CorpusID:13384652},
  series = {IMPACT '12},
}
@INPROCEEDINGS{widesa,
  author={Dai, Tuo and Shi, Bizhao and Luo, Guojie},
  booktitle={2024 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  title={WideSA: A High Array Utilization Mapping Scheme for Uniform Recurrences on ACAP}, 
  year={2024},
  pages={1-6},
  keywords={Signal processing algorithms;Computer architecture;Signal processing;Throughput;Systolic arrays;Space exploration;Computational efficiency;Mapping;Re-configurable Array Architecture;Versal ACAP},
  doi={10.48550/arXiv.2401.16792},
  link={http://192.168.0.2:5012/2024/03/25/797ce7da8e3cf731fa0d04f7303e50f0a18a3a56/},
}
@article{xdnn,
  title={Accelerating DNNs with Xilinx Alveo Accelerator Cards},
  author={Xilinx},
  journal={Tech. Rep.},
  year={2018}
}
@inproceedings{zinenko,
  author = {Zinenko, Oleksandr and Verdoolaege, Sven and Reddy, Chandan and Shirako, Jun and Grosser, Tobias and Sarkar, Vivek and Cohen, Albert},
  title = {Modeling the conflicting demands of parallelism and Temporal/Spatial locality in affine scheduling},
  year = {2018},
  isbn = {9781450356442},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  link = {https://doi.org/10.1145/3178372.3179507},
  doi = {10.1145/3178372.3179507},
  abstract = {The construction of effective loop nest optimizers and parallelizers remains challenging despite decades of work in the area. Due to the increasing diversity of loop-intensive applications and to the complex memory/computation hierarchies in modern processors, optimization heuristics are pulled towards conflicting goals, highlighting the lack of a systematic approach to optimizing locality and parallelism. Acknowledging these conflicting demands on loop nest optimization, we propose an algorithmic template capable of modeling the multi-level parallelism and the temporal/spatial locality of multiprocessors and accelerators. This algorithmic template orchestrates a collection of parameterizable, linear optimization problems over a polyhedral space of semantics-preserving transformations. While the overall problem is not convex, effective algorithms can be derived from this template delivering unprecedented performance portability over GPU and multicore CPU. We discuss the rationale for this algorithmic template and validate it on representative computational kernels/benchmarks.},
  booktitle = {Proceedings of the 27th International Conference on Compiler Construction},
  pages = {3–13},
  numpages = {11},
  keywords = {Polyhedral Model, Compiler Optimizations},
  location = {Vienna, Austria},
  series = {CC '18},
}
